{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tnpp-diaparse.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/Turku-neural-parser-pipeline/blob/master/docs/tnpp_diaparse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhS5gcnQMUUX"
      },
      "source": [
        "# Turku Neural Parser Pipeline\n",
        "\n",
        "* A mini-tutorial of the latest version of the parser pipeline\n",
        "* Make sure to run it with GPU enabled (Runtime -> Change runtime type -> GPU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhGCL2O2GaQn"
      },
      "source": [
        "# Modules\n",
        "\n",
        "## Segmentation\n",
        "\n",
        "* Tokenization and sentence segmentation happens jointly, and is implemented using the UDPipe library\n",
        "* Machine-learned sequence classification model\n",
        "\n",
        "## PoS and morphological tagging\n",
        "\n",
        "* A BERT-based classification model\n",
        "* Joint prediction of PoS and Tags\n",
        "* Implemented in Pytorch Lightning\n",
        "\n",
        "## Dependency parsing\n",
        "\n",
        "* Parsing is done using the [diaparser](https://github.com/Unipisa/diaparser) parser\n",
        "* A BERT-based model, implemented in Torch\n",
        "\n",
        "## Lemmatization\n",
        "\n",
        "* Lemmatization is a sequence-to-sequence model\n",
        "* Wordform + Tags -> Lemma\n",
        "* Fully machine-learned\n",
        "* Implemented using OpenNMT (a machine translation library)\n",
        "\n",
        "## GPU\n",
        "\n",
        "* Current accuracy far beyond previous versions of this pipeline\n",
        "* Cost: computationally intense deep neural network models\n",
        "* Small tests and examples can run on CPU, but any non-trivial amount of text needs a GPU accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sOFiRoqJ7fw"
      },
      "source": [
        "# INSTALL\n",
        "\n",
        "* git clone the code\n",
        "* cd to the directory\n",
        "* and install all requirements\n",
        "* this does take its time, the parser leans on quite large libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKPUaw73JEwK",
        "outputId": "4f8e1417-5787-4a9d-8f26-3a28efc68614"
      },
      "source": [
        "!git clone https://github.com/TurkuNLP/Turku-neural-parser-pipeline.git\n",
        "%cd Turku-neural-parser-pipeline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Turku-neural-parser-pipeline'...\n",
            "remote: Enumerating objects: 1277, done.\u001b[K\n",
            "remote: Counting objects: 100% (318/318), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 1277 (delta 188), reused 304 (delta 179), pack-reused 959\u001b[K\n",
            "Receiving objects: 100% (1277/1277), 367.26 KiB | 4.32 MiB/s, done.\n",
            "Resolving deltas: 100% (746/746), done.\n",
            "/content/Turku-neural-parser-pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Colab -specific installation\n",
        "\n",
        "* Let us install only what we need for Google Colab\n",
        "* Import pytorch_lighting to avoid a problem later\n",
        "* Normally, you would install using `requirements.txt`"
      ],
      "metadata": {
        "id": "JOf9yqeX48mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install ufal.udpipe configargparse transformers \"OpenNMT-py>=1.2.0\" \"git+https://github.com/TurkuNLP/diaparser.git@master\" \"pytorch_lightning<1.5.0\" \"torchmetrics<=0.7.3\""
      ],
      "metadata": {
        "id": "WwIxPhRT0366",
        "outputId": "35b4737f-92d2-4dfc-8398-7d8cc2aa0f5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/TurkuNLP/diaparser.git@master\n",
            "  Cloning https://github.com/TurkuNLP/diaparser.git (to revision master) to /tmp/pip-req-build-gkx7df6o\n",
            "  Running command git clone -q https://github.com/TurkuNLP/diaparser.git /tmp/pip-req-build-gkx7df6o\n",
            "Collecting ufal.udpipe\n",
            "  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 9.1 MB/s \n",
            "\u001b[?25hCollecting OpenNMT-py>=1.2.0\n",
            "  Downloading OpenNMT_py-2.2.0-py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 44.7 MB/s \n",
            "\u001b[?25hCollecting pytorch_lightning<1.5.0\n",
            "  Downloading pytorch_lightning-1.4.9-py3-none-any.whl (925 kB)\n",
            "\u001b[K     |████████████████████████████████| 925 kB 42.2 MB/s \n",
            "\u001b[?25hCollecting torchmetrics<=0.7.3\n",
            "  Downloading torchmetrics-0.7.3-py3-none-any.whl (398 kB)\n",
            "\u001b[K     |████████████████████████████████| 398 kB 44.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from diaparser==1.1.2) (1.11.0+cu113)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from diaparser==1.1.2) (3.2.5)\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.4.0-py3-none-any.whl (574 kB)\n",
            "\u001b[K     |████████████████████████████████| 574 kB 47.2 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.4\n",
            "  Downloading numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 44.3 MB/s \n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.23\n",
            "  Downloading pyonmttok-1.31.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.6 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py>=1.2.0) (3.13)\n",
            "Requirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py>=1.2.0) (2.8.0)\n",
            "Collecting waitress\n",
            "  Downloading waitress-2.1.1-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py>=1.2.0) (1.1.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py>=1.2.0) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py>=1.2.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py>=1.2.0) (4.64.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 41.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning<1.5.0) (4.2.0)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 45.9 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 33.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning<1.5.0) (21.3)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 38.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning<1.5.0) (3.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (1.44.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py>=1.2.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py>=1.2.0) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py>=1.2.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py>=1.2.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py>=1.2.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py>=1.2.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py>=1.2.0) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py>=1.2.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py>=1.2.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py>=1.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py>=1.2.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py>=1.2.0) (3.2.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 35.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning<1.5.0) (21.4.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 42.3 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning<1.5.0) (2.0.12)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py>=1.2.0) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py>=1.2.0) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py>=1.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->OpenNMT-py>=1.2.0) (2.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 50.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: diaparser, future, ufal.udpipe, sacremoses, emoji\n",
            "  Building wheel for diaparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diaparser: filename=diaparser-1.1.2-py3-none-any.whl size=69625 sha256=0be988fdf97151330e1594bc1c77bffc816a9950da2067c66d4999a3130cf8c4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gvl4pg0m/wheels/ab/b2/69/46ac4042140de04a78b6f2c4b8950d4cff3b8da05a4174fcab\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=0c96d06ff38c95c3f8178c6ff1b46107a3eccf60cf1b763bf976f594db07cb86\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626668 sha256=5801a9e8cc57ca45d655c96ed620cfac4559e4ee202d9184648190549d46bbcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/b5/8e/3da091629a21ce2d10bf90759d0cb034ba10a5cf7a01e83d64\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=af8bccc5d6f081e1a6ff3a1200c7224287eafde155f596232e13e92c54960757\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=393ac0ce07b9145cc14eba5eccfe85f2efcc5091988518b35f93f4ff74521723\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built diaparser future ufal.udpipe sacremoses emoji\n",
            "Installing collected packages: pyyaml, multidict, frozenlist, yarl, tokenizers, sacremoses, numpy, huggingface-hub, asynctest, async-timeout, aiosignal, transformers, sentencepiece, pyDeprecate, fsspec, emoji, aiohttp, waitress, torchtext, torchmetrics, stanza, pyonmttok, future, configargparse, ufal.udpipe, pytorch-lightning, OpenNMT-py, diaparser\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.19.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed OpenNMT-py-2.2.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 configargparse-1.5.3 diaparser-1.1.2 emoji-1.7.0 frozenlist-1.3.0 fsspec-2022.3.0 future-0.18.2 huggingface-hub-0.5.1 multidict-6.0.2 numpy-1.19.4 pyDeprecate-0.3.1 pyonmttok-1.31.0 pytorch-lightning-1.4.9 pyyaml-6.0 sacremoses-0.0.53 sentencepiece-0.1.96 stanza-1.4.0 tokenizers-0.12.1 torchmetrics-0.7.3 torchtext-0.5.0 transformers-4.18.0 ufal.udpipe-1.2.0.3 waitress-2.1.1 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install \"torchmetrics<=0.7.3\""
      ],
      "metadata": {
        "id": "grTFJpBEzFdi",
        "outputId": "96342379-ea80-4d26-c68f-a65bdd057251",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics<=0.7.3 in /usr/local/lib/python3.7/dist-packages (0.7.3)\n",
            "Requirement already satisfied: pyDeprecate==0.3.* in /usr/local/lib/python3.7/dist-packages (from torchmetrics<=0.7.3) (0.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics<=0.7.3) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics<=0.7.3) (1.19.4)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics<=0.7.3) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics<=0.7.3) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics<=0.7.3) (3.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning\n"
      ],
      "metadata": {
        "id": "Dc5xDFLL1D1x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrsm2c8AKLeF"
      },
      "source": [
        "# FETCH MODEL\n",
        "\n",
        "* At present, only the Finnish (fi_tdt_dia) and English (en_ewt_dia) models are available for the most recent diaparser-based version of the pipeline\n",
        "* Models documented here: http://turkunlp.org/Turku-neural-parser-pipeline/models.html\n",
        "* ...the remainder of UD languages is in the works..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UviA2z6DKWYv",
        "outputId": "9a96f86e-d0e6-4df9-e033-0b91228395fe"
      },
      "source": [
        "!python3 fetch_models.py fi_tdt_dia"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from fi_tdt_dia and unpacking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9gJOGYLKx6u"
      },
      "source": [
        "* Note: this might take a while, the model is quite large (>1GB)\n",
        "* The above command created the directory `models_fi_tdt_dia` with the model\n",
        "* The file `models_fi_tdt_dia/pipelines.yaml` defines all the possible pipelines for the parser in this model\n",
        "* The `parse_plaintext` is the correct choice in most situations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWztsJw8LIeY"
      },
      "source": [
        "# PARSE IN PYTHON\n",
        "\n",
        "* You need to load and start the pipeline of choice\n",
        "* Like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p6um1idLVun",
        "outputId": "c0935f4f-57ae-4fd7-ec39-ddeac93ccd0d"
      },
      "source": [
        "from tnparser.pipeline import read_pipelines, Pipeline\n",
        "\n",
        "# What pipelines do we have for the Finnish model?\n",
        "available_pipelines=read_pipelines(\"models_fi_tdt_dia/pipelines.yaml\")               # {pipeline_name -> its steps}\n",
        "# This is a dictionary, its keys are the pipelines\n",
        "print(list(available_pipelines.keys()))\n",
        "# Instantiate one of the pipelines\n",
        "p=Pipeline(available_pipelines[\"parse_plaintext\"])    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['parse_plaintext', 'tag_plaintext', 'parse_sentlines', 'parse_wslines', 'parse_conllu', 'tokenize', 'parse_noisytext']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/decorators.py:66: LightningDeprecationWarning: The `@auto_move_data` decorator is deprecated in v1.3 and will be removed in v1.5. Please use `trainer.predict` instead for inference. The decorator was applied to `predict`\n",
            "  \"The `@auto_move_data` decorator is deprecated in v1.3 and will be removed in v1.5.\"\n",
            "INFO:root:Loading model from /content/Turku-neural-parser-pipeline/models_fi_tdt_dia/Tagger/best.ckpt\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "Lemmatizer device: gpu / 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNkU44JEIWJT",
        "outputId": "23081d4b-9ad9-459e-e115-657448e96b82"
      },
      "source": [
        "txt_in=\"Minulla on söpö koira. Se haukkuu, syö makkaraa, jahtaa oravia ja tsillailee kanssani!\"\n",
        "parsed=p.parse(txt_in)\n",
        "print(parsed)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# newdoc\n",
            "# newpar\n",
            "# sent_id = 1\n",
            "# text = Minulla on söpö koira.\n",
            "1\tMinulla\tminä\tPRON\t_\tCase=Ade|Number=Sing|Person=1|PronType=Prs\t0\troot\t_\t_\n",
            "2\ton\tolla\tAUX\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t1\tcop:own\t_\t_\n",
            "3\tsöpö\tsöpö\tADJ\t_\tCase=Nom|Degree=Pos|Number=Sing\t4\tamod\t_\t_\n",
            "4\tkoira\tkoira\tNOUN\t_\tCase=Nom|Number=Sing\t1\tnsubj:cop\t_\tSpaceAfter=No\n",
            "5\t.\t.\tPUNCT\t_\t_\t1\tpunct\t_\t_\n",
            "\n",
            "# sent_id = 2\n",
            "# text = Se haukkuu, syö makkaraa, jahtaa oravia ja tsillailee kanssani!\n",
            "1\tSe\tse\tPRON\t_\tCase=Nom|Number=Sing|PronType=Dem\t2\tnsubj\t_\t_\n",
            "2\thaukkuu\thaukkua\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\tSpaceAfter=No\n",
            "3\t,\t,\tPUNCT\t_\t_\t4\tpunct\t_\t_\n",
            "4\tsyö\tsyödä\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t2\tconj\t_\t_\n",
            "5\tmakkaraa\tmakkara\tNOUN\t_\tCase=Par|Number=Sing\t4\tobj\t_\tSpaceAfter=No\n",
            "6\t,\t,\tPUNCT\t_\t_\t7\tpunct\t_\t_\n",
            "7\tjahtaa\tjahtaa\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t2\tconj\t_\t_\n",
            "8\toravia\torava\tNOUN\t_\tCase=Par|Number=Plur\t7\tobj\t_\t_\n",
            "9\tja\tja\tCCONJ\t_\t_\t10\tcc\t_\t_\n",
            "10\ttsillailee\ttsillailla\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t2\tconj\t_\t_\n",
            "11\tkanssani\tkanssa\tADV\t_\tNumber[psor]=Sing|Person[psor]=1\t10\tadvmod\t_\tSpaceAfter=No\n",
            "12\t!\t!\tPUNCT\t_\t_\t2\tpunct\t_\tSpacesAfter=\\n\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvaSQcqfL6Ru"
      },
      "source": [
        "# Parsing more data\n",
        "\n",
        "* You might have many files with data you need to parse\n",
        "* If you have massive documents, it makes sense to split them into manageable pieces\n",
        "* Here is a basic example of how to achieve that\n",
        "* You can download an example zip file I prepared from here: [http://dl.turkunlp.org/.ginter/news_test_data.zip](http://dl.turkunlp.org/.ginter/news_test_data.zip)\n",
        "* Or simply upload your own\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oZ4OxYnVeII",
        "outputId": "01e9c79c-9e3c-43f4-af24-408eebb948dd"
      },
      "source": [
        "#Remember this notebook uses Turku-neural-parser-pipeline as its working directory\n",
        "!wget http://dl.turkunlp.org/.ginter/news_test_data.zip\n",
        "!unzip news_test_data.zip #will unzip some 60 files into ./test_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-10 11:55:52--  http://dl.turkunlp.org/.ginter/news_test_data.zip\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 136098 (133K) [application/zip]\n",
            "Saving to: ‘news_test_data.zip’\n",
            "\n",
            "news_test_data.zip  100%[===================>] 132.91K   308KB/s    in 0.4s    \n",
            "\n",
            "2022-05-10 11:55:53 (308 KB/s) - ‘news_test_data.zip’ saved [136098/136098]\n",
            "\n",
            "Archive:  news_test_data.zip\n",
            "   creating: test_data/\n",
            "  inflating: test_data/yle_news_0061.txt  \n",
            "  inflating: test_data/yle_news_0053.txt  \n",
            "  inflating: test_data/yle_news_0052.txt  \n",
            "  inflating: test_data/yle_news_0050.txt  \n",
            "  inflating: test_data/yle_news_0017.txt  \n",
            "  inflating: test_data/yle_news_0044.txt  \n",
            "  inflating: test_data/yle_news_0001.txt  \n",
            "  inflating: test_data/yle_news_0005.txt  \n",
            "  inflating: test_data/yle_news_0009.txt  \n",
            "  inflating: test_data/yle_news_0051.txt  \n",
            "  inflating: test_data/yle_news_0029.txt  \n",
            "  inflating: test_data/yle_news_0046.txt  \n",
            "  inflating: test_data/yle_news_0021.txt  \n",
            "  inflating: test_data/yle_news_0059.txt  \n",
            "  inflating: test_data/yle_news_0043.txt  \n",
            "  inflating: test_data/yle_news_0019.txt  \n",
            "  inflating: test_data/yle_news_0014.txt  \n",
            "  inflating: test_data/yle_news_0063.txt  \n",
            "  inflating: test_data/yle_news_0006.txt  \n",
            "  inflating: test_data/yle_news_0007.txt  \n",
            "  inflating: test_data/yle_news_0026.txt  \n",
            "  inflating: test_data/yle_news_0023.txt  \n",
            "  inflating: test_data/yle_news_0047.txt  \n",
            "  inflating: test_data/yle_news_0065.txt  \n",
            "  inflating: test_data/yle_news_0042.txt  \n",
            "  inflating: test_data/yle_news_0060.txt  \n",
            "  inflating: test_data/yle_news_0003.txt  \n",
            "  inflating: test_data/yle_news_0045.txt  \n",
            "  inflating: test_data/yle_news_0058.txt  \n",
            "  inflating: test_data/yle_news_0038.txt  \n",
            "  inflating: test_data/yle_news_0056.txt  \n",
            "  inflating: test_data/yle_news_0004.txt  \n",
            "  inflating: test_data/yle_news_0037.txt  \n",
            "  inflating: test_data/yle_news_0027.txt  \n",
            "  inflating: test_data/yle_news_0036.txt  \n",
            "  inflating: test_data/yle_news_0057.txt  \n",
            "  inflating: test_data/yle_news_0034.txt  \n",
            "  inflating: test_data/yle_news_0033.txt  \n",
            "  inflating: test_data/yle_news_0025.txt  \n",
            "  inflating: test_data/yle_news_0018.txt  \n",
            "  inflating: test_data/yle_news_0066.txt  \n",
            "  inflating: test_data/yle_news_0024.txt  \n",
            "  inflating: test_data/yle_news_0064.txt  \n",
            "  inflating: test_data/yle_news_0062.txt  \n",
            "  inflating: test_data/yle_news_0000.txt  \n",
            "  inflating: test_data/yle_news_0008.txt  \n",
            "  inflating: test_data/yle_news_0049.txt  \n",
            "  inflating: test_data/yle_news_0041.txt  \n",
            "  inflating: test_data/yle_news_0039.txt  \n",
            "  inflating: test_data/yle_news_0020.txt  \n",
            "  inflating: test_data/yle_news_0022.txt  \n",
            "  inflating: test_data/yle_news_0055.txt  \n",
            "  inflating: test_data/yle_news_0054.txt  \n",
            "  inflating: test_data/yle_news_0048.txt  \n",
            "  inflating: test_data/yle_news_0040.txt  \n",
            "  inflating: test_data/yle_news_0010.txt  \n",
            "  inflating: test_data/yle_news_0011.txt  \n",
            "  inflating: test_data/yle_news_0030.txt  \n",
            "  inflating: test_data/yle_news_0012.txt  \n",
            "  inflating: test_data/yle_news_0013.txt  \n",
            "  inflating: test_data/yle_news_0016.txt  \n",
            "  inflating: test_data/yle_news_0028.txt  \n",
            "  inflating: test_data/yle_news_0015.txt  \n",
            "  inflating: test_data/yle_news_0035.txt  \n",
            "  inflating: test_data/yle_news_0002.txt  \n",
            "  inflating: test_data/yle_news_0031.txt  \n",
            "  inflating: test_data/yle_news_0032.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eshlzXZAX_5h"
      },
      "source": [
        "* Now we have 67 text files in `test_data` and we would like to parse them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsm11zrsVtyD",
        "outputId": "710cc461-158c-444d-bfc0-b997267251fb"
      },
      "source": [
        "import glob #allows listing files\n",
        "import tqdm #progress bar\n",
        "\n",
        "all_files=glob.glob(\"test_data/*.txt\") #list all files we need\n",
        "\n",
        "for file_name in tqdm.tqdm(all_files):\n",
        "    txt=open(file_name).read() #read the file\n",
        "    parsed=p.parse(txt) #parse it\n",
        "    with open(file_name.replace(\".txt\",\".conllu\"),\"wt\") as f_out: #open output file\n",
        "        f_out.write(parsed) #and write out the result"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 67/67 [01:18<00:00,  1.17s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPnxeTeVZNfa"
      },
      "source": [
        "* there are now parsed conllu files under `test_data` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eT4Rc_2ZbmN",
        "outputId": "1abaabb9-c3a5-4c2d-c497-1cdb772bfa18"
      },
      "source": [
        "# Basic stats of the parsed files\n",
        "!echo \"Sentences:\" ; cat test_data/*.conllu | grep -Pc '^1\\t'\n",
        "!echo \"Tokens:\" ; cat test_data/*.conllu | grep -Pc '^[0-9]+\\t'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "2689\n",
            "Tokens:\n",
            "35681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-mfmb-2ZzPO"
      },
      "source": [
        "* Now we yet need to pack and download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8WphD8WZ2Lz",
        "outputId": "1fd6cd4b-5568-4287-d8c5-cb315e766595"
      },
      "source": [
        "!zip parsed.zip test_data/*.conllu"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: test_data/yle_news_0000.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0001.conllu (deflated 75%)\n",
            "  adding: test_data/yle_news_0002.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0003.conllu (deflated 73%)\n",
            "  adding: test_data/yle_news_0004.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0005.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0006.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0007.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0008.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0009.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0010.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0011.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0012.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0013.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0014.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0015.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0016.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0017.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0018.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0019.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0020.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0021.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0022.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0023.conllu (deflated 75%)\n",
            "  adding: test_data/yle_news_0024.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0025.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0026.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0027.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0028.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0029.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0030.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0031.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0032.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0033.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0034.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0035.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0036.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0037.conllu (deflated 76%)\n",
            "  adding: test_data/yle_news_0038.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0039.conllu (deflated 74%)\n",
            "  adding: test_data/yle_news_0040.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0041.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0042.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0043.conllu (deflated 72%)\n",
            "  adding: test_data/yle_news_0044.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0045.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0046.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0047.conllu (deflated 67%)\n",
            "  adding: test_data/yle_news_0048.conllu (deflated 72%)\n",
            "  adding: test_data/yle_news_0049.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0050.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0051.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0052.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0053.conllu (deflated 83%)\n",
            "  adding: test_data/yle_news_0054.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0055.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0056.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0057.conllu (deflated 75%)\n",
            "  adding: test_data/yle_news_0058.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0059.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0060.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0061.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0062.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0063.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0064.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0065.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0066.conllu (deflated 79%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqIMDbdFZ88B"
      },
      "source": [
        "...and download the `parsed.zip` file and you're good to go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQjvSbNxOmPV"
      },
      "source": [
        "# Models\n",
        "\n",
        "* Universal Dependencies models\n",
        "* A handful of specialized models (e.g. biomedical English)\n",
        "* Training new models not particularly difficult, documentation for the diaparser-based pipeline training in the works\n",
        "\n",
        "# Failure modes\n",
        "\n",
        "* Generally this is a pretty stable parser, it was used to parse some hundreds of millions of sentences successfully\n",
        "* Most failures stem from the bleeding-edge libraries we are forced to use; these keep changing rapidly\n",
        "* Backward-incompatible, breaking changes are very common\n",
        "* Google Colab environment regularly upgraded to newest versions of many common libraries, and this might break some dependencies\n",
        "\n",
        "In case of failure:\n",
        "\n",
        "* Runtime -> Factory reset runtime, try again\n",
        "* Check that you are on a GPU runtime, large files might still take long to parse -> split your data into more manageable pieces\n",
        "* Ping Filip Ginter or Jenna Kanerva with as good a description of the problem as possible\n"
      ]
    }
  ]
}
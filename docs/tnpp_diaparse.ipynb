{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tnpp-diaparse.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/Turku-neural-parser-pipeline/blob/master/docs/tnpp_diaparse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhS5gcnQMUUX"
      },
      "source": [
        "# Turku Neural Parser Pipeline\n",
        "\n",
        "* A mini-tutorial of the latest version of the parser pipeline\n",
        "* Make sure to run it with GPU enabled (Runtime -> Change runtime type -> GPU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhGCL2O2GaQn"
      },
      "source": [
        "# Modules\n",
        "\n",
        "## Segmentation\n",
        "\n",
        "* Tokenization and sentence segmentation happens jointly, and is implemented using the UDPipe library\n",
        "* Machine-learned sequence classification model\n",
        "\n",
        "## PoS and morphological tagging\n",
        "\n",
        "* A BERT-based classification model\n",
        "* Joint prediction of PoS and Tags\n",
        "* Implemented in Pytorch Lightning\n",
        "\n",
        "## Dependency parsing\n",
        "\n",
        "* Parsing is done using the [diaparser](https://github.com/Unipisa/diaparser) parser\n",
        "* A BERT-based model, implemented in Torch\n",
        "\n",
        "## Lemmatization\n",
        "\n",
        "* Lemmatization is a sequence-to-sequence model\n",
        "* Wordform + Tags -> Lemma\n",
        "* Fully machine-learned\n",
        "* Implemented using OpenNMT (a machine translation library)\n",
        "\n",
        "## GPU\n",
        "\n",
        "* Current accuracy far beyond previous versions of this pipeline\n",
        "* Cost: computationally intense deep neural network models\n",
        "* Small tests and examples can run on CPU, but any non-trivial amount of text needs a GPU accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sOFiRoqJ7fw"
      },
      "source": [
        "# INSTALL\n",
        "\n",
        "* git clone the code\n",
        "* cd to the directory\n",
        "* and install all requirements\n",
        "* this does take its time, the parser leans on quite large libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKPUaw73JEwK",
        "outputId": "bc844786-1b08-47de-ffa6-71bc4369233a"
      },
      "source": [
        "!git clone https://github.com/TurkuNLP/Turku-neural-parser-pipeline.git\n",
        "%cd Turku-neural-parser-pipeline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Turku-neural-parser-pipeline'...\n",
            "remote: Enumerating objects: 1267, done.\u001b[K\n",
            "remote: Counting objects: 100% (308/308), done.\u001b[K\n",
            "remote: Compressing objects: 100% (242/242), done.\u001b[K\n",
            "remote: Total 1267 (delta 179), reused 138 (delta 65), pack-reused 959\u001b[K\n",
            "Receiving objects: 100% (1267/1267), 365.03 KiB | 5.21 MiB/s, done.\n",
            "Resolving deltas: 100% (737/737), done.\n",
            "/content/Turku-neural-parser-pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Colab -specific installation\n",
        "\n",
        "* Let us install only what we need for Google Colab\n",
        "* Import pytorch_lighting to avoid a problem later\n",
        "* Normally, you would install using `requirements.txt`"
      ],
      "metadata": {
        "id": "JOf9yqeX48mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install ufal.udpipe configargparse transformers \"OpenNMT-py>=1.2.0\" \"git+https://github.com/TurkuNLP/diaparser.git@master\" \"pytorch_lightning<1.5.0\" torchmetrics"
      ],
      "metadata": {
        "id": "WwIxPhRT0366",
        "outputId": "5708feca-94fc-44a8-d100-feb2d03abe42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/TurkuNLP/diaparser.git@master\n",
            "  Cloning https://github.com/TurkuNLP/diaparser.git (to revision master) to /tmp/pip-req-build-amgjrnj3\n",
            "  Running command git clone -q https://github.com/TurkuNLP/diaparser.git /tmp/pip-req-build-amgjrnj3\n",
            "Collecting ufal.udpipe\n",
            "  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 59.0 MB/s \n",
            "\u001b[?25hCollecting OpenNMT-py>=1.2.0\n",
            "  Downloading OpenNMT_py-2.2.0-py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 78.2 MB/s \n",
            "\u001b[?25hCollecting pytorch_lightning<1.5.0\n",
            "  Downloading pytorch_lightning-1.4.9-py3-none-any.whl (925 kB)\n",
            "\u001b[K     |████████████████████████████████| 925 kB 62.2 MB/s \n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-0.7.0-py3-none-any.whl (396 kB)\n",
            "\u001b[K     |████████████████████████████████| 396 kB 83.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from diaparser==1.1.2) (1.10.0+cu111)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from diaparser==1.1.2) (3.2.5)\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.3.0-py3-none-any.whl (432 kB)\n",
            "\u001b[K     |████████████████████████████████| 432 kB 76.0 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.4\n",
            "  Downloading numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.23\n",
            "  Downloading pyonmttok-1.30.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 28.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py>=1.2.0) (2.7.0)\n",
            "Collecting waitress\n",
            "  Downloading waitress-2.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py>=1.2.0) (1.1.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py>=1.2.0) (3.13)\n",
            "Collecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py>=1.2.0) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 60.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py>=1.2.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->OpenNMT-py>=1.2.0) (4.62.3)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 74.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning<1.5.0) (21.3)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 82.7 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 61.7 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning<1.5.0) (3.10.0.2)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 64.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning<1.5.0) (3.0.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (1.35.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (1.43.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3->OpenNMT-py>=1.2.0) (3.3.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py>=1.2.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py>=1.2.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py>=1.2.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py>=1.2.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py>=1.2.0) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.3->OpenNMT-py>=1.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py>=1.2.0) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py>=1.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py>=1.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py>=1.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.5.0->OpenNMT-py>=1.2.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py>=1.2.0) (3.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 67.0 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 75.1 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 82.7 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning<1.5.0) (2.0.10)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 81.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning<1.5.0) (21.4.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py>=1.2.0) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py>=1.2.0) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->OpenNMT-py>=1.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->OpenNMT-py>=1.2.0) (2.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 74.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: diaparser, future, ufal.udpipe, emoji\n",
            "  Building wheel for diaparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diaparser: filename=diaparser-1.1.2-py3-none-any.whl size=69625 sha256=a06b5163c2022d6c92796e7597f198942d71077e0b37857b17ab9574460effb1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dichh5ge/wheels/ab/b2/69/46ac4042140de04a78b6f2c4b8950d4cff3b8da05a4174fcab\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=57059e6ee0f18b5444de5e6b2c95fab8612b68befd7af818af03c3b0cc50d1aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626660 sha256=35b74fcf19f6d028e460e3900b91dbb6ea7720126f778e663f0721f943101a37\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/b5/8e/3da091629a21ce2d10bf90759d0cb034ba10a5cf7a01e83d64\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=5031f19ffb25a344821d3be21dca5c0b3ffed31baa6f4844cb49cfa0702beafd\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "Successfully built diaparser future ufal.udpipe emoji\n",
            "Installing collected packages: multidict, frozenlist, yarl, pyyaml, asynctest, async-timeout, aiosignal, tokenizers, sentencepiece, sacremoses, pyDeprecate, numpy, huggingface-hub, fsspec, emoji, aiohttp, waitress, transformers, torchtext, torchmetrics, stanza, pyonmttok, future, configargparse, ufal.udpipe, pytorch-lightning, OpenNMT-py, diaparser\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.11.0\n",
            "    Uninstalling torchtext-0.11.0:\n",
            "      Successfully uninstalled torchtext-0.11.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed OpenNMT-py-2.2.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 configargparse-1.5.3 diaparser-1.1.2 emoji-1.6.3 frozenlist-1.3.0 fsspec-2022.1.0 future-0.18.2 huggingface-hub-0.4.0 multidict-5.2.0 numpy-1.19.4 pyDeprecate-0.3.1 pyonmttok-1.30.0 pytorch-lightning-1.4.9 pyyaml-6.0 sacremoses-0.0.47 sentencepiece-0.1.96 stanza-1.3.0 tokenizers-0.10.3 torchmetrics-0.7.0 torchtext-0.5.0 transformers-4.15.0 ufal.udpipe-1.2.0.3 waitress-2.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning\n"
      ],
      "metadata": {
        "id": "Dc5xDFLL1D1x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrsm2c8AKLeF"
      },
      "source": [
        "# FETCH MODEL\n",
        "\n",
        "* At present, only the Finnish (fi_tdt_dia) and English (en_ewt_dia) models are available for the most recent diaparser-based version of the pipeline\n",
        "* Models documented here: http://turkunlp.org/Turku-neural-parser-pipeline/models.html\n",
        "* ...the remainder of UD languages is in the works..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UviA2z6DKWYv",
        "outputId": "0a7a2117-1ee0-4866-90aa-fa51dfeceef3"
      },
      "source": [
        "!python3 fetch_models.py fi_tdt_dia"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from fi_tdt_dia and unpacking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9gJOGYLKx6u"
      },
      "source": [
        "* Note: this might take a while, the model is quite large (>1GB)\n",
        "* The above command created the directory `models_fi_tdt_dia` with the model\n",
        "* The file `models_fi_tdt_dia/pipelines.yaml` defines all the possible pipelines for the parser in this model\n",
        "* The `parse_plaintext` is the correct choice in most situations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWztsJw8LIeY"
      },
      "source": [
        "# PARSE IN PYTHON\n",
        "\n",
        "* You need to load and start the pipeline of choice\n",
        "* Like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p6um1idLVun",
        "outputId": "4f00d011-0eb1-4797-fb1e-d184e357e008"
      },
      "source": [
        "from tnparser.pipeline import read_pipelines, Pipeline\n",
        "\n",
        "# What pipelines do we have for the Finnish model?\n",
        "available_pipelines=read_pipelines(\"models_fi_tdt_dia/pipelines.yaml\")               # {pipeline_name -> its steps}\n",
        "# This is a dictionary, its keys are the pipelines\n",
        "print(list(available_pipelines.keys()))\n",
        "# Instantiate one of the pipelines\n",
        "p=Pipeline(available_pipelines[\"parse_plaintext\"])    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['parse_plaintext', 'tag_plaintext', 'parse_sentlines', 'parse_wslines', 'parse_conllu', 'tokenize', 'parse_noisytext']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/decorators.py:66: LightningDeprecationWarning: The `@auto_move_data` decorator is deprecated in v1.3 and will be removed in v1.5. Please use `trainer.predict` instead for inference. The decorator was applied to `predict`\n",
            "  \"The `@auto_move_data` decorator is deprecated in v1.3 and will be removed in v1.5.\"\n",
            "INFO:root:Loading model from /content/Turku-neural-parser-pipeline/models_fi_tdt_dia/Tagger/best.ckpt\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:338: UserWarning: Trying to unpickle estimator LabelEncoder from version 0.24.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  UserWarning,\n",
            "Lemmatizer device: gpu / 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNkU44JEIWJT",
        "outputId": "c8ab8493-0126-48a2-960a-3f0075883997"
      },
      "source": [
        "txt_in=\"Minulla on söpö koira. Se haukkuu, syö makkaraa, jahtaa oravia ja tsillailee kanssani!\"\n",
        "parsed=p.parse(txt_in)\n",
        "print(parsed)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# newdoc\n",
            "# newpar\n",
            "# sent_id = 1\n",
            "# text = Minulla on söpö koira.\n",
            "1\tMinulla\tminä\tPRON\t_\tCase=Ade|Number=Sing|Person=1|PronType=Prs\t0\troot\t_\t_\n",
            "2\ton\tolla\tAUX\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t1\tcop:own\t_\t_\n",
            "3\tsöpö\tsöpö\tADJ\t_\tCase=Nom|Degree=Pos|Number=Sing\t4\tamod\t_\t_\n",
            "4\tkoira\tkoira\tNOUN\t_\tCase=Nom|Number=Sing\t1\tnsubj:cop\t_\tSpaceAfter=No\n",
            "5\t.\t.\tPUNCT\t_\t_\t1\tpunct\t_\t_\n",
            "\n",
            "# sent_id = 2\n",
            "# text = Se haukkuu, syö makkaraa, jahtaa oravia ja tsillailee kanssani!\n",
            "1\tSe\tse\tPRON\t_\tCase=Nom|Number=Sing|PronType=Dem\t2\tnsubj\t_\t_\n",
            "2\thaukkuu\thaukkua\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\tSpaceAfter=No\n",
            "3\t,\t,\tPUNCT\t_\t_\t4\tpunct\t_\t_\n",
            "4\tsyö\tsyödä\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t2\tconj\t_\t_\n",
            "5\tmakkaraa\tmakkara\tNOUN\t_\tCase=Par|Number=Sing\t4\tobj\t_\tSpaceAfter=No\n",
            "6\t,\t,\tPUNCT\t_\t_\t7\tpunct\t_\t_\n",
            "7\tjahtaa\tjahtaa\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t2\tconj\t_\t_\n",
            "8\toravia\torava\tNOUN\t_\tCase=Par|Number=Plur\t7\tobj\t_\t_\n",
            "9\tja\tja\tCCONJ\t_\t_\t10\tcc\t_\t_\n",
            "10\ttsillailee\ttsillailla\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t2\tconj\t_\t_\n",
            "11\tkanssani\tkanssa\tADV\t_\tNumber[psor]=Sing|Person[psor]=1\t10\tadvmod\t_\tSpaceAfter=No\n",
            "12\t!\t!\tPUNCT\t_\t_\t2\tpunct\t_\tSpacesAfter=\\n\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvaSQcqfL6Ru"
      },
      "source": [
        "# Parsing more data\n",
        "\n",
        "* You might have many files with data you need to parse\n",
        "* If you have massive documents, it makes sense to split them into manageable pieces\n",
        "* Here is a basic example of how to achieve that\n",
        "* You can download an example zip file I prepared from here: [http://dl.turkunlp.org/.ginter/news_test_data.zip](http://dl.turkunlp.org/.ginter/news_test_data.zip)\n",
        "* Or simply upload your own\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oZ4OxYnVeII",
        "outputId": "2df5ff59-2173-468c-cef1-17acd054c3c1"
      },
      "source": [
        "#Remember this notebook uses Turku-neural-parser-pipeline as its working directory\n",
        "!wget http://dl.turkunlp.org/.ginter/news_test_data.zip\n",
        "!unzip news_test_data.zip #will unzip some 60 files into ./test_data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-19 10:58:24--  http://dl.turkunlp.org/.ginter/news_test_data.zip\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 136098 (133K) [application/zip]\n",
            "Saving to: ‘news_test_data.zip’\n",
            "\n",
            "news_test_data.zip  100%[===================>] 132.91K   269KB/s    in 0.5s    \n",
            "\n",
            "2022-01-19 10:58:25 (269 KB/s) - ‘news_test_data.zip’ saved [136098/136098]\n",
            "\n",
            "Archive:  news_test_data.zip\n",
            "   creating: test_data/\n",
            "  inflating: test_data/yle_news_0061.txt  \n",
            "  inflating: test_data/yle_news_0053.txt  \n",
            "  inflating: test_data/yle_news_0052.txt  \n",
            "  inflating: test_data/yle_news_0050.txt  \n",
            "  inflating: test_data/yle_news_0017.txt  \n",
            "  inflating: test_data/yle_news_0044.txt  \n",
            "  inflating: test_data/yle_news_0001.txt  \n",
            "  inflating: test_data/yle_news_0005.txt  \n",
            "  inflating: test_data/yle_news_0009.txt  \n",
            "  inflating: test_data/yle_news_0051.txt  \n",
            "  inflating: test_data/yle_news_0029.txt  \n",
            "  inflating: test_data/yle_news_0046.txt  \n",
            "  inflating: test_data/yle_news_0021.txt  \n",
            "  inflating: test_data/yle_news_0059.txt  \n",
            "  inflating: test_data/yle_news_0043.txt  \n",
            "  inflating: test_data/yle_news_0019.txt  \n",
            "  inflating: test_data/yle_news_0014.txt  \n",
            "  inflating: test_data/yle_news_0063.txt  \n",
            "  inflating: test_data/yle_news_0006.txt  \n",
            "  inflating: test_data/yle_news_0007.txt  \n",
            "  inflating: test_data/yle_news_0026.txt  \n",
            "  inflating: test_data/yle_news_0023.txt  \n",
            "  inflating: test_data/yle_news_0047.txt  \n",
            "  inflating: test_data/yle_news_0065.txt  \n",
            "  inflating: test_data/yle_news_0042.txt  \n",
            "  inflating: test_data/yle_news_0060.txt  \n",
            "  inflating: test_data/yle_news_0003.txt  \n",
            "  inflating: test_data/yle_news_0045.txt  \n",
            "  inflating: test_data/yle_news_0058.txt  \n",
            "  inflating: test_data/yle_news_0038.txt  \n",
            "  inflating: test_data/yle_news_0056.txt  \n",
            "  inflating: test_data/yle_news_0004.txt  \n",
            "  inflating: test_data/yle_news_0037.txt  \n",
            "  inflating: test_data/yle_news_0027.txt  \n",
            "  inflating: test_data/yle_news_0036.txt  \n",
            "  inflating: test_data/yle_news_0057.txt  \n",
            "  inflating: test_data/yle_news_0034.txt  \n",
            "  inflating: test_data/yle_news_0033.txt  \n",
            "  inflating: test_data/yle_news_0025.txt  \n",
            "  inflating: test_data/yle_news_0018.txt  \n",
            "  inflating: test_data/yle_news_0066.txt  \n",
            "  inflating: test_data/yle_news_0024.txt  \n",
            "  inflating: test_data/yle_news_0064.txt  \n",
            "  inflating: test_data/yle_news_0062.txt  \n",
            "  inflating: test_data/yle_news_0000.txt  \n",
            "  inflating: test_data/yle_news_0008.txt  \n",
            "  inflating: test_data/yle_news_0049.txt  \n",
            "  inflating: test_data/yle_news_0041.txt  \n",
            "  inflating: test_data/yle_news_0039.txt  \n",
            "  inflating: test_data/yle_news_0020.txt  \n",
            "  inflating: test_data/yle_news_0022.txt  \n",
            "  inflating: test_data/yle_news_0055.txt  \n",
            "  inflating: test_data/yle_news_0054.txt  \n",
            "  inflating: test_data/yle_news_0048.txt  \n",
            "  inflating: test_data/yle_news_0040.txt  \n",
            "  inflating: test_data/yle_news_0010.txt  \n",
            "  inflating: test_data/yle_news_0011.txt  \n",
            "  inflating: test_data/yle_news_0030.txt  \n",
            "  inflating: test_data/yle_news_0012.txt  \n",
            "  inflating: test_data/yle_news_0013.txt  \n",
            "  inflating: test_data/yle_news_0016.txt  \n",
            "  inflating: test_data/yle_news_0028.txt  \n",
            "  inflating: test_data/yle_news_0015.txt  \n",
            "  inflating: test_data/yle_news_0035.txt  \n",
            "  inflating: test_data/yle_news_0002.txt  \n",
            "  inflating: test_data/yle_news_0031.txt  \n",
            "  inflating: test_data/yle_news_0032.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eshlzXZAX_5h"
      },
      "source": [
        "* Now we have 67 text files in `test_data` and we would like to parse them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsm11zrsVtyD",
        "outputId": "5d0ba4be-1323-47fa-be0f-4ee7daf9d0d5"
      },
      "source": [
        "import glob #allows listing files\n",
        "import tqdm #progress bar\n",
        "\n",
        "all_files=glob.glob(\"test_data/*.txt\") #list all files we need\n",
        "\n",
        "for file_name in tqdm.tqdm(all_files):\n",
        "    txt=open(file_name).read() #read the file\n",
        "    parsed=p.parse(txt) #parse it\n",
        "    with open(file_name.replace(\".txt\",\".conllu\"),\"wt\") as f_out: #open output file\n",
        "        f_out.write(parsed) #and write out the result"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 67/67 [00:43<00:00,  1.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPnxeTeVZNfa"
      },
      "source": [
        "* there are now parsed conllu files under `test_data` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eT4Rc_2ZbmN",
        "outputId": "bf039203-c6ff-4b47-cdf5-8c89adf2db65"
      },
      "source": [
        "# Basic stats of the parsed files\n",
        "!echo \"Sentences:\" ; cat test_data/*.conllu | grep -Pc '^1\\t'\n",
        "!echo \"Tokens:\" ; cat test_data/*.conllu | grep -Pc '^[0-9]+\\t'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences:\n",
            "2689\n",
            "Tokens:\n",
            "35681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-mfmb-2ZzPO"
      },
      "source": [
        "* Now we yet need to pack and download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8WphD8WZ2Lz",
        "outputId": "3644f699-93d1-48ea-a076-1ce91d6a9708"
      },
      "source": [
        "!zip parsed.zip test_data/*.conllu"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: test_data/yle_news_0000.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0001.conllu (deflated 75%)\n",
            "  adding: test_data/yle_news_0002.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0003.conllu (deflated 73%)\n",
            "  adding: test_data/yle_news_0004.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0005.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0006.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0007.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0008.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0009.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0010.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0011.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0012.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0013.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0014.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0015.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0016.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0017.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0018.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0019.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0020.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0021.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0022.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0023.conllu (deflated 75%)\n",
            "  adding: test_data/yle_news_0024.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0025.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0026.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0027.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0028.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0029.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0030.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0031.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0032.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0033.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0034.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0035.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0036.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0037.conllu (deflated 76%)\n",
            "  adding: test_data/yle_news_0038.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0039.conllu (deflated 74%)\n",
            "  adding: test_data/yle_news_0040.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0041.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0042.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0043.conllu (deflated 72%)\n",
            "  adding: test_data/yle_news_0044.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0045.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0046.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0047.conllu (deflated 67%)\n",
            "  adding: test_data/yle_news_0048.conllu (deflated 72%)\n",
            "  adding: test_data/yle_news_0049.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0050.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0051.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0052.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0053.conllu (deflated 83%)\n",
            "  adding: test_data/yle_news_0054.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0055.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0056.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0057.conllu (deflated 75%)\n",
            "  adding: test_data/yle_news_0058.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0059.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0060.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0061.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0062.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0063.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0064.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0065.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0066.conllu (deflated 79%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqIMDbdFZ88B"
      },
      "source": [
        "...and download the `parsed.zip` file and you're good to go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQjvSbNxOmPV"
      },
      "source": [
        "# Models\n",
        "\n",
        "* Universal Dependencies models\n",
        "* A handful of specialized models (e.g. biomedical English)\n",
        "* Training new models not particularly difficult, documentation for the diaparser-based pipeline training in the works\n",
        "\n",
        "# Failure modes\n",
        "\n",
        "* Generally this is a pretty stable parser, it was used to parse some hundreds of millions of sentences successfully\n",
        "* Most failures stem from the bleeding-edge libraries we are forced to use; these keep changing rapidly\n",
        "* Backward-incompatible, breaking changes are very common\n",
        "* Google Colab environment regularly upgraded to newest versions of many common libraries, and this might break some dependencies\n",
        "\n",
        "In case of failure:\n",
        "\n",
        "* Runtime -> Factory reset runtime, try again\n",
        "* Check that you are on a GPU runtime, large files might still take long to parse -> split your data into more manageable pieces\n",
        "* Ping Filip Ginter or Jenna Kanerva with as good a description of the problem as possible\n"
      ]
    }
  ]
}
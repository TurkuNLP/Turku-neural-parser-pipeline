{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tnpp-diaparse.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNulOHUN78+zO5MUt5ywzI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/Turku-neural-parser-pipeline/blob/master/docs/tnpp_diaparse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhS5gcnQMUUX"
      },
      "source": [
        "# Turku Neural Parser Pipeline\n",
        "\n",
        "* A mini-tutorial of the latest version of the parser pipeline\n",
        "* Make sure to run it with GPU enabled (Runtime -> Change runtime type -> GPU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhGCL2O2GaQn"
      },
      "source": [
        "# Modules\n",
        "\n",
        "## Segmentation\n",
        "\n",
        "* Tokenization and sentence segmentation happens jointly, and is implemented using the UDPipe library\n",
        "* Machine-learned sequence classification model\n",
        "\n",
        "## PoS and morphological tagging\n",
        "\n",
        "* A BERT-based classification model\n",
        "* Joint prediction of PoS and Tags\n",
        "* Implemented in Pytorch Lightning\n",
        "\n",
        "## Dependency parsing\n",
        "\n",
        "* Parsing is done using the [diaparser](https://github.com/Unipisa/diaparser) parser\n",
        "* A BERT-based model, implemented in Torch\n",
        "\n",
        "## Lemmatization\n",
        "\n",
        "* Lemmatization is a sequence-to-sequence model\n",
        "* Wordform + Tags -> Lemma\n",
        "* Fully machine-learned\n",
        "* Implemented using OpenNMT (a machine translation library)\n",
        "\n",
        "## GPU\n",
        "\n",
        "* Current accuracy far beyond previous versions of this pipeline\n",
        "* Cost: computationally intense deep neural network models\n",
        "* Small tests and examples can run on CPU, but any non-trivial amount of text needs a GPU accelerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sOFiRoqJ7fw"
      },
      "source": [
        "# INSTALL\n",
        "\n",
        "* git clone the code\n",
        "* cd to the directory\n",
        "* and install all requirements\n",
        "* this does take its time, the parser leans on quite large libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKPUaw73JEwK",
        "outputId": "72b75270-69db-4148-d7da-29ef86436f8e"
      },
      "source": [
        "!git clone https://github.com/TurkuNLP/Turku-neural-parser-pipeline.git\n",
        "%cd Turku-neural-parser-pipeline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Turku-neural-parser-pipeline'...\n",
            "remote: Enumerating objects: 1148, done.\u001b[K\n",
            "remote: Counting objects: 100% (189/189), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 1148 (delta 104), reused 95 (delta 43), pack-reused 959\u001b[K\n",
            "Receiving objects: 100% (1148/1148), 336.37 KiB | 2.55 MiB/s, done.\n",
            "Resolving deltas: 100% (662/662), done.\n",
            "/content/Turku-neural-parser-pipeline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RybqhQ5YMpJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7071bd9-38dc-49b8-dfd6-a7f0951151f3"
      },
      "source": [
        "#I like to upgrade these first\n",
        "!python3 -m pip install --upgrade pip\n",
        "!python3 -m pip install --upgrade setuptools"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.2.4-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-21.2.4\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok1lM5IOJmU3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c30d36-9c37-4cf2-f35d-8f83c95d962d"
      },
      "source": [
        "!python3 -m pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.9.0+cu102)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.19.5)\n",
            "Collecting ufal.udpipe\n",
            "  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.13)\n",
            "Collecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.2-py3-none-any.whl (20 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 44.3 MB/s \n",
            "\u001b[?25hCollecting OpenNMT-py>=1.2.0\n",
            "  Downloading OpenNMT_py-2.1.2-py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 50.8 MB/s \n",
            "\u001b[?25hCollecting diaparser\n",
            "  Downloading diaparser-1.1.2-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.4.4-py3-none-any.whl (918 kB)\n",
            "\u001b[K     |████████████████████████████████| 918 kB 41.5 MB/s \n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-0.5.0-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 56.4 MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.24\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.9->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 2)) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->-r requirements.txt (line 3)) (2.11.3)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 8)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 8)) (4.62.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 8)) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 8)) (4.6.4)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 8)) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 40.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-5.4-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<3,>=2.3 in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (2.6.0)\n",
            "Collecting waitress==1.4.4\n",
            "  Downloading waitress-1.4.4-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting torchtext==0.5.0\n",
            "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.23\n",
            "  Downloading pyonmttok-1.26.4-cp37-cp37m-manylinux1_x86_64.whl (14.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.3 MB 151 kB/s \n",
            "\u001b[?25hCollecting flask\n",
            "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting OpenNMT-py>=1.2.0\n",
            "  Downloading OpenNMT_py-2.1.1-py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 50.8 MB/s \n",
            "\u001b[?25h  Downloading OpenNMT_py-2.1.0-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 56.5 MB/s \n",
            "\u001b[?25h  Downloading OpenNMT_py-2.0.1-py3-none-any.whl (207 kB)\n",
            "\u001b[K     |████████████████████████████████| 207 kB 54.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 54.4 MB/s \n",
            "\u001b[?25hCollecting OpenNMT-py>=1.2.0\n",
            "  Downloading OpenNMT_py-2.0.0-py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 50.4 MB/s \n",
            "\u001b[?25h  Downloading OpenNMT_py-1.2.0-py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 57.8 MB/s \n",
            "\u001b[?25hCollecting waitress\n",
            "  Downloading waitress-2.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (0.16.0)\n",
            "Collecting torchtext==0.4.0\n",
            "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from diaparser->-r requirements.txt (line 10)) (3.2.5)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 46 kB/s \n",
            "\u001b[?25hCollecting stanza\n",
            "  Downloading stanza-1.2.3-py3-none-any.whl (342 kB)\n",
            "\u001b[K     |████████████████████████████████| 342 kB 50.5 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 39.5 MB/s \n",
            "\u001b[?25hCollecting future\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->-r requirements.txt (line 13)) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->-r requirements.txt (line 13)) (1.4.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->-r requirements.txt (line 3)) (2.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->-r requirements.txt (line 8)) (2.4.7)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (3.3.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (1.39.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (0.4.5)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (3.17.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (1.34.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (1.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3->OpenNMT-py>=1.2.0->-r requirements.txt (line 9)) (3.1.1)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning->-r requirements.txt (line 11)) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 54.0 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r requirements.txt (line 8)) (3.5.0)\n",
            "Building wheels for collected packages: ufal.udpipe, future\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626737 sha256=5c1508a03dc6bc92e6d92718b2b6f878040e2806e91a61dcbfaeb1031666c745\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/b5/8e/3da091629a21ce2d10bf90759d0cb034ba10a5cf7a01e83d64\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=068606d6e2efe52a05a2d2cfa695bd3a192cc622cdce05b486ae658f2b152bf1\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built ufal.udpipe future\n",
            "Installing collected packages: multidict, yarl, async-timeout, tokenizers, sacremoses, pyyaml, numpy, huggingface-hub, fsspec, aiohttp, waitress, transformers, torchtext, torchmetrics, threadpoolctl, stanza, pyonmttok, pyDeprecate, future, configargparse, ufal.udpipe, scikit-learn, pytorch-lightning, OpenNMT-py, diaparser\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed OpenNMT-py-1.2.0 aiohttp-3.7.4.post0 async-timeout-3.0.1 configargparse-1.5.2 diaparser-1.1.2 fsspec-2021.7.0 future-0.18.2 huggingface-hub-0.0.12 multidict-5.1.0 numpy-1.19.4 pyDeprecate-0.3.1 pyonmttok-1.26.4 pytorch-lightning-1.4.4 pyyaml-5.4.1 sacremoses-0.0.45 scikit-learn-0.24.2 stanza-1.2.3 threadpoolctl-2.2.0 tokenizers-0.10.3 torchmetrics-0.5.0 torchtext-0.4.0 transformers-4.9.2 ufal.udpipe-1.2.0.3 waitress-2.0.0 yarl-1.6.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMAjETYgkNMT",
        "outputId": "563849ad-4b9b-4303-ac8f-a50c5d983012"
      },
      "source": [
        "#this is something we need to do for now, hopefully it eases when\n",
        "#the next version of OpenNMT comes out\n",
        "!python3 -m pip install --no-deps OpenNMT-py==2.1.2 "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting OpenNMT-py==2.1.2\n",
            "  Using cached OpenNMT_py-2.1.2-py3-none-any.whl (212 kB)\n",
            "Installing collected packages: OpenNMT-py\n",
            "  Attempting uninstall: OpenNMT-py\n",
            "    Found existing installation: OpenNMT-py 1.2.0\n",
            "    Uninstalling OpenNMT-py-1.2.0:\n",
            "      Successfully uninstalled OpenNMT-py-1.2.0\n",
            "Successfully installed OpenNMT-py-2.1.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrsm2c8AKLeF"
      },
      "source": [
        "# FETCH MODEL\n",
        "\n",
        "* At present, only the Finnish (fi_tdt_dia) and English (en_ewt_dia) models are available for the most recent diaparser-based version of the pipeline\n",
        "* Models documented here: http://turkunlp.org/Turku-neural-parser-pipeline/models.html\n",
        "* ...the remainder of UD languages is in the works..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UviA2z6DKWYv",
        "outputId": "47611812-a470-44ce-b66d-54f6bf46b6f3"
      },
      "source": [
        "!python3 fetch_models.py fi_tdt_dia"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading from fi_tdt_dia and unpacking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9gJOGYLKx6u"
      },
      "source": [
        "* Note: this might take a while, the model is quite large (>1GB)\n",
        "* The above command created the directory `models_fi_tdt_dia` with the model\n",
        "* The file `models_fi_tdt_dia/pipelines.yaml` defines all the possible pipelines for the parser in this model\n",
        "* The `parse_plaintext` is the correct choice in most situations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWztsJw8LIeY"
      },
      "source": [
        "# PARSE IN PYTHON\n",
        "\n",
        "* You need to load and start the pipeline of choice\n",
        "* Like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p6um1idLVun",
        "outputId": "4f46635e-d6d4-461e-aa14-88f41fc031fb"
      },
      "source": [
        "from tnparser.pipeline import read_pipelines, Pipeline\n",
        "\n",
        "# What pipelines do we have for the Finnish model?\n",
        "available_pipelines=read_pipelines(\"models_fi_tdt_dia/pipelines.yaml\")               # {pipeline_name -> its steps}\n",
        "# This is a dictionary, its keys are the pipelines\n",
        "print(list(available_pipelines.keys()))\n",
        "# Instantiate one of the pipelines\n",
        "p=Pipeline(available_pipelines[\"parse_plaintext\"])    "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['parse_plaintext', 'tag_plaintext', 'parse_sentlines', 'parse_wslines', 'parse_conllu', 'tokenize', 'parse_noisytext']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/decorators.py:66: LightningDeprecationWarning: The `@auto_move_data` decorator is deprecated in v1.3 and will be removed in v1.5. Please use `trainer.predict` instead for inference. The decorator was applied to `predict`\n",
            "  \"The `@auto_move_data` decorator is deprecated in v1.3 and will be removed in v1.5.\"\n",
            "INFO:root:Loading model from /content/Turku-neural-parser-pipeline/models_fi_tdt_dia/Tagger/best.ckpt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNkU44JEIWJT",
        "outputId": "b4753dcf-1dae-4c54-d6f3-80d66327f3e5"
      },
      "source": [
        "txt_in=\"Minulla on söpö koira. Se haukkuu, syö makkaraa, jahtaa oravia ja tsillailee kanssani!\"\n",
        "parsed=p.parse(txt_in)\n",
        "print(parsed)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# newdoc\n",
            "# newpar\n",
            "# sent_id = 1\n",
            "# text = Minulla on söpö koira.\n",
            "1\tMinulla\tminä\tPRON\t_\tCase=Ade|Number=Sing|Person=1|PronType=Prs\t0\troot\t_\t_\n",
            "2\ton\tolla\tAUX\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t1\tcop:own\t_\t_\n",
            "3\tsöpö\tsöpö\tADJ\t_\tCase=Nom|Degree=Pos|Number=Sing\t4\tamod\t_\t_\n",
            "4\tkoira\tkoira\tNOUN\t_\tCase=Nom|Number=Sing\t1\tnsubj:cop\t_\tSpaceAfter=No\n",
            "5\t.\t.\tPUNCT\t_\t_\t1\tpunct\t_\t_\n",
            "\n",
            "# sent_id = 2\n",
            "# text = Se haukkuu, syö makkaraa, jahtaa oravia ja tsillailee kanssani!\n",
            "1\tSe\tse\tPRON\t_\tCase=Nom|Number=Sing|PronType=Dem\t2\tnsubj\t_\t_\n",
            "2\thaukkuu\thaukkua\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\tSpaceAfter=No\n",
            "3\t,\t,\tPUNCT\t_\t_\t4\tpunct\t_\t_\n",
            "4\tsyö\tsyödä\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t2\tconj\t_\t_\n",
            "5\tmakkaraa\tmakkara\tNOUN\t_\tCase=Par|Number=Sing\t4\tobj\t_\tSpaceAfter=No\n",
            "6\t,\t,\tPUNCT\t_\t_\t7\tpunct\t_\t_\n",
            "7\tjahtaa\tjahtaa\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t2\tconj\t_\t_\n",
            "8\toravia\torava\tNOUN\t_\tCase=Par|Number=Plur\t7\tobj\t_\t_\n",
            "9\tja\tja\tCCONJ\t_\t_\t10\tcc\t_\t_\n",
            "10\ttsillailee\ttsillailla\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t2\tconj\t_\t_\n",
            "11\tkanssani\tkanssa\tADV\t_\tNumber[psor]=Sing|Person[psor]=1\t10\tadvmod\t_\tSpaceAfter=No\n",
            "12\t!\t!\tPUNCT\t_\t_\t2\tpunct\t_\tSpacesAfter=\\n\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvaSQcqfL6Ru"
      },
      "source": [
        "# Parsing more data\n",
        "\n",
        "* You might have many files with data you need to parse\n",
        "* If you have massive documents, it makes sense to split them into manageable pieces\n",
        "* Here is a basic example of how to achieve that\n",
        "* You can download an example zip file I prepared from here: [http://bionlp-www.utu.fi/.ginter/news_test_data.zip](http://bionlp-www.utu.fi/.ginter/news_test_data.zip)\n",
        "* Or simply upload your own\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oZ4OxYnVeII",
        "outputId": "3382163a-841e-4c83-b60c-1f6b78dff239"
      },
      "source": [
        "#Remember this notebook uses Turku-neural-parser-pipeline as its working directory\n",
        "!wget http://bionlp-www.utu.fi/.ginter/news_test_data.zip\n",
        "!unzip news_test_data.zip #will unzip some 60 files into ./test_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-24 17:22:50--  http://bionlp-www.utu.fi/.ginter/news_test_data.zip\n",
            "Resolving bionlp-www.utu.fi (bionlp-www.utu.fi)... 130.232.253.44\n",
            "Connecting to bionlp-www.utu.fi (bionlp-www.utu.fi)|130.232.253.44|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 136098 (133K) [application/zip]\n",
            "Saving to: ‘news_test_data.zip’\n",
            "\n",
            "news_test_data.zip  100%[===================>] 132.91K  94.4KB/s    in 1.4s    \n",
            "\n",
            "2021-08-24 17:22:52 (94.4 KB/s) - ‘news_test_data.zip’ saved [136098/136098]\n",
            "\n",
            "Archive:  news_test_data.zip\n",
            "   creating: test_data/\n",
            "  inflating: test_data/yle_news_0061.txt  \n",
            "  inflating: test_data/yle_news_0053.txt  \n",
            "  inflating: test_data/yle_news_0052.txt  \n",
            "  inflating: test_data/yle_news_0050.txt  \n",
            "  inflating: test_data/yle_news_0017.txt  \n",
            "  inflating: test_data/yle_news_0044.txt  \n",
            "  inflating: test_data/yle_news_0001.txt  \n",
            "  inflating: test_data/yle_news_0005.txt  \n",
            "  inflating: test_data/yle_news_0009.txt  \n",
            "  inflating: test_data/yle_news_0051.txt  \n",
            "  inflating: test_data/yle_news_0029.txt  \n",
            "  inflating: test_data/yle_news_0046.txt  \n",
            "  inflating: test_data/yle_news_0021.txt  \n",
            "  inflating: test_data/yle_news_0059.txt  \n",
            "  inflating: test_data/yle_news_0043.txt  \n",
            "  inflating: test_data/yle_news_0019.txt  \n",
            "  inflating: test_data/yle_news_0014.txt  \n",
            "  inflating: test_data/yle_news_0063.txt  \n",
            "  inflating: test_data/yle_news_0006.txt  \n",
            "  inflating: test_data/yle_news_0007.txt  \n",
            "  inflating: test_data/yle_news_0026.txt  \n",
            "  inflating: test_data/yle_news_0023.txt  \n",
            "  inflating: test_data/yle_news_0047.txt  \n",
            "  inflating: test_data/yle_news_0065.txt  \n",
            "  inflating: test_data/yle_news_0042.txt  \n",
            "  inflating: test_data/yle_news_0060.txt  \n",
            "  inflating: test_data/yle_news_0003.txt  \n",
            "  inflating: test_data/yle_news_0045.txt  \n",
            "  inflating: test_data/yle_news_0058.txt  \n",
            "  inflating: test_data/yle_news_0038.txt  \n",
            "  inflating: test_data/yle_news_0056.txt  \n",
            "  inflating: test_data/yle_news_0004.txt  \n",
            "  inflating: test_data/yle_news_0037.txt  \n",
            "  inflating: test_data/yle_news_0027.txt  \n",
            "  inflating: test_data/yle_news_0036.txt  \n",
            "  inflating: test_data/yle_news_0057.txt  \n",
            "  inflating: test_data/yle_news_0034.txt  \n",
            "  inflating: test_data/yle_news_0033.txt  \n",
            "  inflating: test_data/yle_news_0025.txt  \n",
            "  inflating: test_data/yle_news_0018.txt  \n",
            "  inflating: test_data/yle_news_0066.txt  \n",
            "  inflating: test_data/yle_news_0024.txt  \n",
            "  inflating: test_data/yle_news_0064.txt  \n",
            "  inflating: test_data/yle_news_0062.txt  \n",
            "  inflating: test_data/yle_news_0000.txt  \n",
            "  inflating: test_data/yle_news_0008.txt  \n",
            "  inflating: test_data/yle_news_0049.txt  \n",
            "  inflating: test_data/yle_news_0041.txt  \n",
            "  inflating: test_data/yle_news_0039.txt  \n",
            "  inflating: test_data/yle_news_0020.txt  \n",
            "  inflating: test_data/yle_news_0022.txt  \n",
            "  inflating: test_data/yle_news_0055.txt  \n",
            "  inflating: test_data/yle_news_0054.txt  \n",
            "  inflating: test_data/yle_news_0048.txt  \n",
            "  inflating: test_data/yle_news_0040.txt  \n",
            "  inflating: test_data/yle_news_0010.txt  \n",
            "  inflating: test_data/yle_news_0011.txt  \n",
            "  inflating: test_data/yle_news_0030.txt  \n",
            "  inflating: test_data/yle_news_0012.txt  \n",
            "  inflating: test_data/yle_news_0013.txt  \n",
            "  inflating: test_data/yle_news_0016.txt  \n",
            "  inflating: test_data/yle_news_0028.txt  \n",
            "  inflating: test_data/yle_news_0015.txt  \n",
            "  inflating: test_data/yle_news_0035.txt  \n",
            "  inflating: test_data/yle_news_0002.txt  \n",
            "  inflating: test_data/yle_news_0031.txt  \n",
            "  inflating: test_data/yle_news_0032.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eshlzXZAX_5h"
      },
      "source": [
        "* Now we have 67 text files in `test_data` and we would like to parse them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsm11zrsVtyD",
        "outputId": "84201cbd-b17d-4577-e175-82a6222203f3"
      },
      "source": [
        "import glob #allows listing files\n",
        "import tqdm #progress bar\n",
        "\n",
        "all_files=glob.glob(\"test_data/*.txt\") #list all files we need\n",
        "\n",
        "for file_name in tqdm.tqdm(all_files):\n",
        "    txt=open(file_name).read() #read the file\n",
        "    parsed=p.parse(txt) #parse it\n",
        "    with open(file_name.replace(\".txt\",\".conllu\"),\"wt\") as f_out: #open output file\n",
        "        f_out.write(parsed) #and write out the result"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [01:38<00:00,  1.47s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPnxeTeVZNfa"
      },
      "source": [
        "* there are now parsed conllu files under `test_data` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eT4Rc_2ZbmN",
        "outputId": "f8ba2085-9d62-405a-dacc-0a2bd7b64726"
      },
      "source": [
        "# Basic stats of the parsed files\n",
        "!echo \"Sentences:\" ; cat test_data/*.conllu | grep -Pc '^1\\t'\n",
        "!echo \"Tokens:\" ; cat test_data/*.conllu | grep -Pc '^[0-9]+\\t'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentences:\n",
            "2689\n",
            "Tokens:\n",
            "35681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-mfmb-2ZzPO"
      },
      "source": [
        "* Now we yet need to pack and download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8WphD8WZ2Lz",
        "outputId": "f4140abc-0fc9-4954-c5e9-922704be38aa"
      },
      "source": [
        "!zip parsed.zip test_data/*.conllu"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: test_data/yle_news_0000.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0001.conllu (deflated 75%)\n",
            "  adding: test_data/yle_news_0002.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0003.conllu (deflated 73%)\n",
            "  adding: test_data/yle_news_0004.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0005.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0006.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0007.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0008.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0009.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0010.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0011.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0012.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0013.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0014.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0015.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0016.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0017.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0018.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0019.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0020.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0021.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0022.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0023.conllu (deflated 75%)\n",
            "  adding: test_data/yle_news_0024.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0025.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0026.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0027.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0028.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0029.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0030.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0031.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0032.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0033.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0034.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0035.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0036.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0037.conllu (deflated 76%)\n",
            "  adding: test_data/yle_news_0038.conllu (deflated 82%)\n",
            "  adding: test_data/yle_news_0039.conllu (deflated 75%)\n",
            "  adding: test_data/yle_news_0040.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0041.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0042.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0043.conllu (deflated 72%)\n",
            "  adding: test_data/yle_news_0044.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0045.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0046.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0047.conllu (deflated 67%)\n",
            "  adding: test_data/yle_news_0048.conllu (deflated 72%)\n",
            "  adding: test_data/yle_news_0049.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0050.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0051.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0052.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0053.conllu (deflated 83%)\n",
            "  adding: test_data/yle_news_0054.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0055.conllu (deflated 80%)\n",
            "  adding: test_data/yle_news_0056.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0057.conllu (deflated 75%)\n",
            "  adding: test_data/yle_news_0058.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0059.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0060.conllu (deflated 81%)\n",
            "  adding: test_data/yle_news_0061.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0062.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0063.conllu (deflated 78%)\n",
            "  adding: test_data/yle_news_0064.conllu (deflated 77%)\n",
            "  adding: test_data/yle_news_0065.conllu (deflated 79%)\n",
            "  adding: test_data/yle_news_0066.conllu (deflated 79%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqIMDbdFZ88B"
      },
      "source": [
        "...and download the `parsed.zip` file and you're good to go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQjvSbNxOmPV"
      },
      "source": [
        "# Models\n",
        "\n",
        "* Universal Dependencies models\n",
        "* A handful of specialized models (e.g. biomedical English)\n",
        "* Training new models not particularly difficult, documentation for the diaparser-based pipeline training in the works\n",
        "\n",
        "# Failure modes\n",
        "\n",
        "* Generally this is a pretty stable parser, it was used to parse some hundreds of millions of sentences successfully\n",
        "* Most failures stem from the bleeding-edge libraries we are forced to use; these keep changing rapidly\n",
        "* Backward-incompatible, breaking changes are very common\n",
        "* Google Colab environment regularly upgraded to newest versions of many common libraries, and this might break some dependencies\n",
        "\n",
        "In case of failure:\n",
        "\n",
        "* Runtime -> Factory reset runtime, try again\n",
        "* Check that you are on a GPU runtime, large files might still take long to parse -> split your data into more manageable pieces\n",
        "* Ping Filip Ginter or Jenna Kanerva with as good a description of the problem as possible\n"
      ]
    }
  ]
}